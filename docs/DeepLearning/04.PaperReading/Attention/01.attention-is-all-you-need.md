---
title: Attention is All You Need
tags:
  - Attention
---

# Attention is All You Need

## Abstract

现在主流的序列翻译模型（序列到序列的生成模型）都是基于循环神经网络或者卷积神经网络来做的，也就是包含了 encoder 和 decoder。现阶段最好的模型也是通过注意力机制将 encoder 和 decoder 连接起来。所以我们提出了一种仅用注意力机制的简单结构，完全不使用 CNN 和 RNN。我们的架构在其他领域也非常适用。(使用机器翻译领域作为验证)

## Introduction

循环语言模型和 encoder-decoder 模型是当时使用频率较高的两种解决方案。输出结构化信息比较多的情况下会使用 encoder-decoder 架构来解决这类问题（比较通用）。提出了两种模型的问题：

1. 对于语言模型的解决方案通常会使用 RNN 进行学习，第 $t$ 个词的状态由前面的 第 $t-1$ 个词的状态得到。但这个过程是串行的，难以并行地进行运算，性能较差；并且对于早期的状态信息可能会在训练后期丢失（记忆）。
2. encoder-decoder 也是有类似的问题，结构化的信息较多的情况下彼此之间的具有强相关的关系，难以解耦，导致学习的过程比较粗糙，不利于收敛。

将上述的两种结构进行优化，不使用 RNN，而是用 attention 的方式替代，解决无法并行的问题

## Background

传统的 seq2seq 任务中，

## Conclusion

第一个仅用注意力机制实现序列转换模型，将 encoder-decoder 架构中的循环层替换成多头自注意力。对于翻译任务 Transformer 的训练速度也比循环层或卷积层快。收敛速度快。

我们的模型可以应用其他任务上，在任何基于 attention 的模型都是适用的。Transformer 可以应对不同的数据类型的输入都是适用的，例如图像、音频、视频。
